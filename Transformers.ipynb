{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89483267-e36c-4887-98d0-0ce6ab59e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, Tensor, inf\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2da9951-a1b2-440a-9a7b-41b3a3d780b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175a84bbc05d4fe094d16d44896baf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdc5e9485274625b6b3c1786365a470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"wmt/wmt14\", \"fr-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f176695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class DataEncoding:\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "    \n",
    "    def __tfidf_matrix_generator(self):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        return vectorizer.fit_transform(self.corpus)\n",
    "    \n",
    "    def __positional_encoding(self):\n",
    "        tfidf = self.__tfidf_matrix_generator()\n",
    "        seq_length, no_terms = tfidf.shape\n",
    "        pos_vals = torch.arange(seq_length).unsqueeze(0)\n",
    "        \n",
    "        positional_encoder = nn.Embedding(seq_length, 512)\n",
    "        return positional_encoder(pos_vals)\n",
    "    \n",
    "    def input_data(self):\n",
    "        tfidf_dense = torch.tensor(self.__tfidf_matrix_generator().toarray(), dtype=torch.float32)\n",
    "        return tfidf_dense + self.__positional_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d5a17c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def generate_self_weights_QKV(inpt_mat: Tensor, h: int = 8) -> Tuple[Tensor]:\n",
    "    _, d_model = inpt_mat.shape\n",
    "    Q_weight = nn.Parameter(torch.randn(d_model, d_model))\n",
    "    K_weight = nn.Parameter(torch.randn(d_model, d_model))\n",
    "    V_weight = nn.Parameter(torch.randn(d_model, d_model))\n",
    "    O_weight = nn.Parameter(torch.randn(h*d_model, d_model))\n",
    "    return Q_weight, K_weight, V_weight, O_weight\n",
    "\n",
    "def generate_cross_weights_QKV(inpt_mat: Tensor, otpt_mat: Tensor, h: int = 8) -> Tuple[Tensor]:\n",
    "    _, d_ipt = inpt_mat.shape\n",
    "    _, d_otpt = otpt_mat.shape\n",
    "    Q_weight = nn.Parameter(torch.randn(d_ipt, d_ipt))\n",
    "    K_weight = nn.Parameter(torch.randn(d_ipt, d_ipt))\n",
    "    V_weight = nn.Parameter(torch.randn(d_otpt, d_otpt))\n",
    "    O_weight = nn.Parameter(torch.randn(h*d_otpt, d_otpt))\n",
    "    return Q_weight, K_weight, V_weight, O_weight\n",
    "\n",
    "def layer_norm(ipt: Tensor) -> Tensor:\n",
    "    layer_norm = nn.LayerNorm(ipt.size()[1])\n",
    "    return layer_norm(ipt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4178f15",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_set = ds[\"train\"]\n",
    "translation = train_set[\"translation\"][:10]\n",
    "english = []\n",
    "french = []\n",
    "for sentences in translation:\n",
    "    english.append(sentences[\"en\"])\n",
    "    french.append(sentences[\"fr\"])\n",
    "eng_word2Vec = DataEncoding(english)\n",
    "eng_embedded = eng_word2Vec.input_data()\n",
    "fr_word2Vec = DataEncoding(french)\n",
    "fr_embedded = fr_word2Vec.input_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a97880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, ipt: Tensor):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.ipt = ipt\n",
    "    \n",
    "    def forward(self) -> Tensor:\n",
    "        return layer_norm(nn.ReLU(self.ipt)+self.ipt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, Q: Tensor, K: Tensor, V: Tensor):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.Q = Q\n",
    "        self.K = K\n",
    "        self.V = V\n",
    "    \n",
    "    def simple_attention(self, isMask: bool = False) -> Tensor:\n",
    "        _, d_k = self.K.shape\n",
    "        QK = torch.matmul(self.Q, self.K)\n",
    "        scale = 1/math.sqrt(d_k)\n",
    "        if isMask:\n",
    "            mask = torch.triu(torch.ones_like(QK), diagonal=1).bool()\n",
    "            QK = QK.masked_fill(mask, -inf)\n",
    "        softmax = nn.Softmax.forward(scale * QK)\n",
    "        attention = torch.matmul(softmax, self.V)\n",
    "        return attention    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eba26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, ipt: Tensor):\n",
    "        super(Linear, self).__init__()\n",
    "        self.ipt = ipt\n",
    "    \n",
    "    def forward(self, weight: Tensor) -> Tensor:\n",
    "        return torch.matmul(weight, self.ipt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60222294",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, ipt: Tensor, h: int):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.ipt = ipt\n",
    "        self.h = h\n",
    "        self.W_q, self.W_k, self.W_v, self.W_o = generate_self_weights_QKV(self.ipt)\n",
    "        self.Q = Tensor()\n",
    "        self.K = Tensor()\n",
    "        self.V = Tensor()\n",
    "    \n",
    "    def __init__(self, ipt: Tensor, opt: Tensor, h: int):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.ipt = ipt\n",
    "        self.opt = opt\n",
    "        self.h = h\n",
    "        self.W_q, self.W_k, self.W_v, self.W_o = generate_cross_weights_QKV(self.ipt, self.opt)\n",
    "        self.Q = Tensor()\n",
    "        self.K = Tensor()\n",
    "        self.V = Tensor()\n",
    "        \n",
    "    def set_QKV(self):\n",
    "        self.Q = Linear(self.ipt).forward(self.W_q)\n",
    "        self.K = Linear(self.ipt).forward(self.W_k)\n",
    "        self.V = Linear(self.ipt).forward(self.W_v)\n",
    "        \n",
    "    def forward(self, isMask: bool = False) -> Tensor:\n",
    "        self.set_QKV()\n",
    "        _,d_q = self.Q.shape\n",
    "        _,d_k = self.K.shape\n",
    "        _,d_v = self.V.shape\n",
    "        Q_head = self.Q.view(self.ipt.size(0), self.ipt.size(1), self.h, d_q)\n",
    "        K_head = self.K.view(self.ipt.size(0), self.ipt.size(1), self.h, d_k)\n",
    "        V_head = self.V.view(self.ipt.size(0), self.ipt.size(1), self.h, d_v)\n",
    "        \n",
    "        scale_attention_opt = ScaledDotProductAttention(Q_head, K_head, V_head).simple_attention(isMask)\n",
    "        concat_opt = scale_attention_opt.transpose(1,2).contiguous().view(self.ipt.size(0), self.ipt.size(1), self.h * d_v)\n",
    "        ma_opt = Linear(concat_opt).forward(self.W_o) + self.ipt\n",
    "        return layer_norm(ma_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083d33a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
